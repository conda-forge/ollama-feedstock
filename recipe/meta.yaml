{% set name = "ollama" %}
{% set goname = "github.com/jmorganca/ollama" %}
# DO NOT AUTO MERGE WITHOUT VERIFYING THE GIT_REVISIONS OF ggml AND gguf
{% set version = "0.1.14" %}
{% set ggml_version = "master-9e232f0" %}
{% set ggml_sha256 = "e93334c097c7b6ce17eed47d44934b9354ee29b5ab4f572999547c2d1a2d46ef" %}
{% set gguf_version = "b1610" %}
{% set gguf_sha256 = "4daccb087930b5a7974422c691fb3e86c4c1333e21fd16f031feadf92060aeae" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  - url: https://{{ goname }}/archive/v{{ version }}.tar.gz
    sha256: 789a64e5f8da0cd71ff3ed339fe1b47727a44f6c009c0e3c4c094990b64cbd8b
    folder: .
    patches:
      # Use the same build options from llama.cpp-feedstock
      - 0001-remove-submodule.patch
      - 0001-linux_all-do-not-copy-cuda.patch
      - 0001-darwin_all-metal-pick-device.patch
      - 0002-darwin_all-cmake-flags.patch
      - 0002-win_all-use-ninja-paths.patch
      - 0003-win_all-unhandle-ctrl-z.patch  # [win]
      - 0004-linux-win_all-mkl.patch
      - 0004-darwin_amd64-mkl.patch         # [blas_impl == "mkl"]
  - url: https://github.com/ggerganov/llama.cpp/archive/{{ ggml_version }}.tar.gz
    folder: llm/llama.cpp/ggml
    sha256: {{ ggml_sha256 }}
  - url: https://github.com/ggerganov/llama.cpp/archive/{{ gguf_version }}.tar.gz
    folder: llm/llama.cpp/gguf
    sha256: {{ gguf_sha256 }}

build:
  number: 1
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [(osx and x86_64) or cuda_compiler_version == "None"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [osx and arm64]
  script:
    - git config --system user.email "conda-forge@numfocus.org"
    - git config --system user.name "Conda Forge"
    - git config --global init.defaultBranch main
    {% for framework in ["ggml", "gguf"] %}
    - |
      pushd llm{{ os.sep }}llama.cpp{{ os.sep }}{{ framework }}
      git init
      git add .
      git commit -m "conda-forge build"
      popd
    {% endfor %}

    - export CMAKE_GENERATOR=Ninja    # [unix]
    - set CMAKE_GENERATOR=Ninja       # [win]
    - set CMAKE_GENERATOR_PLATFORM=   # [win]
    - set CMAKE_GENERATOR_TOOLSET=    # [win]

    - export GOFLAGS="'-ldflags=-X=github.com/jmorganca/ollama/version.Version={{ version }} -X=github.com/jmorganca/ollama/server.mode=release'"  # [unix]
    - set GOFLAGS='-ldflags=-X=github.com/jmorganca/ollama/version.Version={{ version }} -X=github.com/jmorganca/ollama/server.mode=release'  # [win]

    - go generate ./... || exit 1
    - go install . || exit 1                                                                                             # [build_platform == target_platform]
    # TODO: This is due to a bug in our go-lang patch 
    #       Error message is go install can't write to GOBIN when cross compiling
    - unset CONDA_GO_COMPILER; GOPATH=$SRC_DIR/gopath go install .; mkdir -p $PREFIX/bin; cp gopath/bin/*/ollama $PREFIX/bin  # [build_platform != target_platform]
    - go-licenses save --save_path licenses ./... || exit 1

  ignore_run_exports_from:
    # llama.cpp server is staticially linked on osx
    - {{ compiler('cxx') }}  # [osx]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                    # [cuda_compiler_version not in (undefined, "None")]
    - {{ compiler('go') }} 1.21
    - go-licenses

    - git
    - cmake
    - ninja
    - pkgconfig                                  # [blas_impl == "mkl"]
  host:
    - cuda-cudart-dev                           # [(cuda_compiler_version or "").startswith("12")]
    - libcublas-dev                             # [(cuda_compiler_version or "").startswith("12")]
    - blas-devel * *{{ blas_impl }}             # [blas_impl == "mkl"]
  run:
    - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version not in (undefined, "None")]
    - cuda-cudart                               # [(cuda_compiler_version or "").startswith("12")]

test:
  commands:
    - ollama --version
    - ollama --help

about:
  home: https://ollama.ai
  summary: Get up and running with Llama 2 and other large language models locally
  license: MIT
  license_family: MIT
  license_file:
    - LICENSE
    - licenses/
  dev_url: https://{{ goname }}

extra:
  recipe-maintainers:
    - sodre
